# LocalLLM-llama-cpp-python-
Chat with your local LLM(gguf) in pycharm or html. You need to install anaconda and pycharm to construct the required environment like llama-cpp-python(cpu or cuda), etc.
