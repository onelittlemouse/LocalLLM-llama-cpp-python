# LocalLLM-llama-cpp-python
Chat with your local LLM(gguf) in console(pycharm IDE) or html(website "http://localhost:1234/v1/chat-ui"). You need to install anaconda and pycharm to construct the required environment like llama-cpp-python(cpu or cuda), etc.
